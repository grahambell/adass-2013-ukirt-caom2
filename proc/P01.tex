% This is the aspauthor.tex LaTeX file
% Copyright 2010, Astronomical Society of the Pacific Conference Series

\documentclass[11pt,twoside]{article}
\usepackage{asp2010}

\resetcounters

\bibliographystyle{asp2010}

\markboth{Bell et al.}{A New Archive of UKIRT Legacy Data at CADC}

\begin{document}

\title{A New Archive of UKIRT Legacy Data at CADC}
\author{Graham~S.~Bell,$^1$ Malcolm~J.~Currie,$^1$ Russell~O.~Redman,$^2$ Maren~Purves,$^1$ and Tim~Jenness$^{1,3}$
\affil{$^1$Joint~Astronomy~Centre, 660~N.~A`oh\={o}k\={u}~Place, Hilo, HI, 96720, U.S.A.}
\affil{$^2$Canadian Astronomy Data Centre, 5071 West Saanich Road, Victoria, BC, V9E 2M7, Canada}
\affil{$^3$Department of Astronomy, Cornell University, Ithaca, NY, 14853, U.S.A.}}

\begin{abstract}
We describe a  new archive of legacy data from the United Kingdom Infrared
Telescope (UKIRT) at the Canadian Astronomy
Data Centre (CADC).  It contains all available data apart from
that from the Wide Field Camera (WFCAM), which is already
accessible from the WFCAM Science Archive.

The desire was to archive the raw data in as close to its original
format as possible.  Where the data followed our current convention
of having a single data file per observation, it was archived
without alteration, except for minor fixes to headers of data in
FITS format to allow it to pass fitsverify and be accepted by CADC.
Some of the older data comprised multiple integrations in separate
files per observation, stored in either NDF or DST format. These
were placed inside HDS container files, and DST files were rearranged
into NDF format.

The metadata describing the observations is
ingested into the CAOM-2 repository via an
intermediate MongoDB header database, which will
also be used to guide the ORAC-DR pipeline
in generating reduced data products.
\end{abstract}

\section{Introduction}

A new archive of legacy data from the United Kingdom Infrared
Telescope (UKIRT) is being constructed at the Canadian Astronomy
Data Centre (CADC).
The archive contains data from UKIRT's cassegrain instruments,
as listed in table~\ref{p01:tab:instruments}.
It excludes data from the Wide Field Camera (WFCAM),
since it is already available in the WFCAM Science Archive
\citep{2008MNRAS.384..637H}
in Edinburgh.
The archive is to includes raw and reduced data,
with the raw data
kept in as close to its original raw format as possible.

\begin{table}[!ht]
\caption{Instruments for which data is included in the new archive}
\smallskip
\begin{center}
\begin{tabular}{lcr}
\tableline
\noalign{\smallskip}
Instrument & Type & Wavelength \\
\noalign{\smallskip}
\tableline
\noalign{\smallskip}
CGS3 & spectrometer  & 10--20\,\micron \\
CGS4 & spectrometer & 1--5\,\micron \\
IRCAM3 & imager & 1--5\,\micron \\
Michelle & imager / spectrometer & 8--25\,\micron  \\
UFTI & imager & 1--2.5\,\micron \\
UIST & imager / spectrometer with integral field unit & 1--5\,\micron  \\
\noalign{\smallskip}
\tableline
\end{tabular}
\end{center}
\label{p01:tab:instruments}
\end{table}

The archive is based on CAOM-2 \citep{2012ASPC..461..339D},
the next generation of the Common Archive Observation Model
currently being developed at CADC.
This standardizes the core parts of the archive
between various ``collections''
and provides
a single ``advanced search'' interface for all of them.
UKIRT has been added as a new collection.

At the start of this project, the raw data had already
been read from archive tapes stored at the
Joint Astronomy Centre and organised on disk.
The process of transferring it to CADC,
reading the headers to generate the metadata
for CAOM-2 and running the data reduction
pipeline
is illustrated in figure~\ref{p01:fig:flowchart}.

\articlefigure[]{P01_f1}{p01:fig:flowchart}{Diagram illustrating
the process of constructing the UKIRT legacy data archive.}

\section{Transfer of Data to CADC}
\label{p01:sec:etransfer}

The raw data were transferred to the archive at CADC
using the
Electronic Transfer system \citep{2005ASPC..347..647M}.
The files were copied to a ``new'' directory in a
staging area where the system agent would
detect them and automate their transfer to the
archive, deleting them when this was complete.
Any files rejected by the agent were instead moved
to one of several ``reject'' directories depending
on the reason.

As the files were staged for transfer,
filenames were standardized following our current convention.
In addition,
some minor adjustments were made,
depending on the format of the file.

\altsubsubsection*{FITS}

Some early raw data from UFTI were written in FITS format.
For transfer to the archive, these files were
required to pass
\texttt{fitsverify},
and this required a few fixes to the headers.
One example was the removal of \texttt{Z} characters
previously used to indicate UTC in date headers.

\altsubsubsection*{NDF}

Much of the data were stored directly in
Starlink extensible N-dimensional Data Format
\citep[see for example,][]{P91_adassxxiii},
which itself is based on the
Starlink Hierarchical Data System (HDS).
In cases where an observation was recorded as
multiple integrations in separate files,
these were stored in a single HDS container file.

\altsubsubsection*{DST}

Data stored in the older Figaro Format
--- which is also based on HDS ---
were converted to NDF.
As for raw NDF files, multiple integrations
were stored in HDS container files.

\section{Ingestion of Metadata into CAOM-2}

The metadata ingestion process was split into two stages.
First the headers were
read from the data files into an intermediate
\texttt{MongoDB} database.
Then for each observation an XML file was written
using the \texttt{pyCAOM2} library,
and sent to the CAOM-2 repository.
Both of these steps were quite time-consuming
and so separating them was very useful
as it allowed them to be developed
independently.

\texttt{MongoDB} was chosen for the intermediate database
as it allowed the
headers to be stored without prior knowledge of their structure
--- they were read in as Python dictionaries and
stored directly in the database.
It was also able easily to handle the large
volume of header data involved (3.8\,GiB).

\section{Data Reduction Pipeline}

The final stage is to run the ORAC-DR pipeline
\citep{1999ASPC..172...11E,2008AN....329..295C}
to generate reduced
data products.
This will use similar infrastructure at CADC as is already
used in the JCMT Science Archive
\citep{2011ASPC..442..203E}
and will reduce a complete night at once allowing
the pipeline to find the required calibrations.
ORAC-DR already been used for all of the instruments
involved except for CGS3.

We are currently testing the pipeline on a range of data,
including that predating its introduction.
Some header changes can be handled by updating the
\texttt{Astro::FITS::HdrTrans} module
\citep[section 2.2]{2008AN....329..295C}
which is used by ORAC-DR to convert
instrument specific header information into
a common generic set of headers.

In other cases the original headers are insufficient,
for example in allowing the pipeline to
select a data reduction recipe and determine the
grouping of the files.
Therefore the intermediate database of header information
can be re-used to identify
the groups of observations to be processed together and to infer
additional headers.

\articlefigure[angle=270,scale=0.5]{P01_f2}{p01:fig:hh111}{Example image
of HH\,111 observed with IRCAM3 in 1997.  Separate integration files
have been merged into a single file per observation
(as described in section~\ref{p01:sec:etransfer}) and the grouping
and additional headers were derived from the intermediate header
database, allowing the pipeline to process the data.}

\section{Conclusions}

\ldots

\bibliography{P01}

\end{document}
